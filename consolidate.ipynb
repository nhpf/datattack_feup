{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATATTACK - Proteção Civil\n",
    "\n",
    " - Catarina Mendes\n",
    " - Carlos Faria\n",
    " - Nicholas Hopf\n",
    " - Rodrigo Gomes\n",
    "\n",
    "---\n",
    "\n",
    "This project aims to provide insight about issues targeted by the portuguese governmental agency \"Proteção Civil\". Those issues will be called \"occurrences\" in this notebook.\n",
    "\n",
    "The scope of our work not only provides insight about past occurrences and the correlation between variables collected in each situation, but also aims to predict the resources needed to effectively solve a future occurrence.\n",
    "\n",
    "At the end, we deployed a web application that serves as an interface for a relevant subset of the models developed in this project, making them freely available for every citizen and government agent to consult. We believe that data transparency can empower the public, so we made an effort to display a simple but functional application.\n",
    "\n",
    "---\n",
    "\n",
    "# CRISP-DM\n",
    "CRISP-DM was the methodology that guided our action plan and allowed us to maintain a consistent workflow throughout the project.\n",
    "\n",
    "At first, we made sure that our understanding about the actions performed by \"Proteção Civil\" was solid, so our following analysis and intervention proposals could be better grounded. After some research, we could get a sense on the economical and social impacts that an effective action could have.\n",
    "\n",
    "Then we looked at the dataset provided. After some discussion about its quantitative and qualitative aspects, we also gathered another dataset with georeferenced information about each Portuguese administrated subdivision, so conversion between geographical coordinates and regions could be efficiently performed and visualized.\n",
    "\n",
    "After that, we proceeded to prepare the provided dataset by removing invalid data and dividing the `Natureza` field into more columns based on the hierarchy involved in its inherent structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:31.227847Z",
     "end_time": "2023-04-23T10:28:31.260872Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and methods\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:31.227995Z",
     "end_time": "2023-04-23T10:28:31.284139Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def datetime_to_hours_int(x):\n",
    "    return [i.total_seconds()/3600 for i in x]\n",
    "\n",
    "def clean_data(df, remove_false_alarms=True) -> pd.DataFrame:\n",
    "    print('df initial length: ', len(df))\n",
    "\n",
    "    data_clean = df[df['Natureza'] != '1']\n",
    "    data_clean = data_clean[data_clean['EstadoOcorrencia'] != '2']\n",
    "    data_clean = data_clean[data_clean['Distrito'] != '0']\n",
    "    data_clean = data_clean[data_clean['Concelho'] != '0']\n",
    "    data_clean = data_clean[data_clean['Numero'] != \"\"\"\\t\\t\\t\\t\\t\\t\\t\\t\"\"\"]\n",
    "    data_clean = pd.concat([data_clean[data_clean['EstadoOcorrencia'] == 'Encerrada'], data_clean[data_clean['EstadoOcorrencia'] == 'Falso Alarme']])\n",
    "\n",
    "    # We replace the \",\" with \".\" to facilitate processing\n",
    "    data_clean['Latitude'] = pd.to_numeric(\n",
    "        data_clean['Latitude'].str.replace(',', '.')\n",
    "    )\n",
    "    data_clean['Longitude'] = pd.to_numeric(\n",
    "        data_clean['Longitude'].str.replace(',', '.')\n",
    "    )    \n",
    "\n",
    "    data_clean['DataOcorrencia'] = pd.to_datetime(data_clean['DataOcorrencia'], format='%d/%m/%Y %H:%M:%S')\n",
    "    data_clean['DataFechoOperacional'] = pd.to_datetime(data_clean['DataFechoOperacional'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    if remove_false_alarms:\n",
    "        data_clean = data_clean[data_clean['EstadoOcorrencia'] != 'Falso Alarme']\n",
    "\n",
    "    data_clean.dropna(inplace=True)\n",
    "    print('Number of occurrences: ', len(data_clean))\n",
    "\n",
    "    data_clean['hh'] = np.multiply(\n",
    "        (data_clean['NumeroOperacionaisAereosEnvolvidos'] + data_clean['NumeroOperacionaisTerrestresEnvolvidos']),\n",
    "        datetime_to_hours_int(data_clean['DataFechoOperacional'] - data_clean['DataOcorrencia'])\n",
    "    )\n",
    "\n",
    "    return data_clean\n",
    "\n",
    "# Define a function to extract category from a string\n",
    "def _extract_category(string, cat_index):\n",
    "    if string == \"nan\" or any(str.isdigit(c) for c in string):\n",
    "        return\n",
    "    else:\n",
    "        parts = string.split(\"/\", 4)\n",
    "        return parts[cat_index-1].strip()\n",
    "\n",
    "\n",
    "# Reformulate categories based on functions defined\n",
    "def parse_categories(data_clean:pd.DataFrame):\n",
    "    data_clean['category1'] = data_clean['Natureza'].astype(str).apply(lambda x: _extract_category(x,cat_index=1))\n",
    "    data_clean['category2'] = data_clean['Natureza'].astype(str).apply(lambda x: _extract_category(x,cat_index=2))\n",
    "    data_clean['category3'] = data_clean['Natureza'].astype(str).apply(lambda x: _extract_category(x,cat_index=3))\n",
    "    \n",
    "    #Consolidate categories with little statistical relevance\n",
    "    relevance_threshold = 100\n",
    "    category_counts = data_clean['category3'].value_counts()\n",
    "    filtered_categories = category_counts[category_counts <= relevance_threshold].index.tolist()\n",
    "    # Replace the filtered categories with a new \"Other\" category\n",
    "    data_clean['category3'] = data_clean['category3'].replace(\n",
    "        to_replace=filtered_categories,\n",
    "        value='Outras ocorrências'\n",
    "    )\n",
    "    \n",
    "    return data_clean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "We also created a new metric called \"men-hour\", to measure the effort of \"Proteção Civil\" personnel during each occurrence.\n",
    "\n",
    "After the dataset preparation, we performed several techniques to visualize the correlation between each variable, if existent, and performed a simple linear regression to verify how high is the correlation between classes of occurrences and their district of origin.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Read the database .CSV\n",
    "occurrences = pd.read_csv(\n",
    "    \"Data/anpc-2016.csv\", sep=\",\", on_bad_lines=\"skip\", low_memory=False\n",
    ")\n",
    "\n",
    "# Get relevant and error-free data\n",
    "occurrences = parse_categories(clean_data(occurrences))\n",
    "\n",
    "# Treat the \n",
    "def hashed_reg(occurrences):\n",
    "    # Encode the categorical variables using feature hashing with HashingVectorizer\n",
    "    num_districts = occurrences.nunique()[\"Distrito\"]\n",
    "    hash_size_increase = 2.5  # Increase number of features to avoid hash collision\n",
    "    district_vectorizer = HashingVectorizer(n_features=num_districts, norm=None, alternate_sign=False)\n",
    "    X_cat = district_vectorizer.fit_transform(occurrences[\"Distrito\"])\n",
    "\n",
    "    num_categories = occurrences.nunique()[\"category2\"]\n",
    "    hash_size_increase = 2.5  # Increase number of features to avoid hash collision\n",
    "    category_vectorizer = HashingVectorizer(n_features=num_categories, norm=None, alternate_sign=False)\n",
    "    X_cat2 = category_vectorizer.fit_transform(occurrences[\"category2\"])\n",
    "\n",
    "    # Combine the encoded categorical variables and the numerical variable\n",
    "    X = pd.concat(\n",
    "        [\n",
    "        pd.DataFrame(X_cat.toarray()), \n",
    "        pd.DataFrame(X_cat2.toarray()),\n",
    "        pd.DataFrame(datetime_to_hours_int(occurrences['DataOcorrencia'] - min(occurrences['DataOcorrencia'])))\n",
    "        ], axis=1\n",
    "    )\n",
    "\n",
    "    # Fit a linear regression model to the data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, occurrences['hh'])\n",
    "\n",
    "    return model.predict(X)\n",
    "\n",
    "def hashed_reg_2(occurrences):\n",
    "    num_districts = occurrences.nunique()[\"Distrito\"]\n",
    "    hash_size_increase = 2.5  # Increase number of features to avoid hash collision\n",
    "    h = FeatureHasher(n_features=int(hash_size_increase * num_districts), input_type=\"string\")\n",
    "    return h.transform(list(occurrences[\"Distrito\"]))\n",
    "\n",
    "hashed_reg(occurrences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:06.188671Z",
     "end_time": "2023-04-23T10:28:08.023361Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Alongside the analysis efforts dedicated to our primary data source, we also created a simple script that loads georreferenced data and associates input coordinates into portuguese political divisions. This script was used later be used in our web application.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:06.192072Z",
     "end_time": "2023-04-23T10:28:10.340525Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# shp -> shapefile (geometry)\n",
    "# dbf -> database (extra info)\n",
    "# prj - projection\n",
    "# shx - index\n",
    "\n",
    "# Get shape of municipalities and parishes\n",
    "portugal_municipalities = gpd.read_file(\"./georeferencing/data_concelhos/concelhos.shp\")\n",
    "\n",
    "# Remove islands\n",
    "portugal_municipalities = portugal_municipalities[portugal_municipalities.NAME_1 != \"Azores\"]\n",
    "portugal_municipalities = portugal_municipalities[portugal_municipalities.NAME_1 != \"Madeira\"]\n",
    "\n",
    "# Get point defined by user (this will be used later in the API)\n",
    "lat, lon = 40.9800516, -8.202641\n",
    "user_point = Point(lon, lat)\n",
    "\n",
    "\n",
    "def get_municipality(point: Point):\n",
    "    for index, row in portugal_municipalities.iterrows():\n",
    "        polygon = row['geometry']\n",
    "        # Check if the point is inside the polygon\n",
    "        if polygon.contains(point):\n",
    "            # Return the index or other properties of the polygon\n",
    "            return {\n",
    "                \"district\": row.NAME_1,\n",
    "                \"municipality\": row.NAME_2,\n",
    "                \"municipality_gdf\": gpd.GeoDataFrame([row], crs=portugal_municipalities.crs)\n",
    "            }\n",
    "\n",
    "\n",
    "def plot_point_in_map(point, municipality_gdf):\n",
    "    point_df = gpd.GeoDataFrame(geometry=[point])\n",
    "    pt_map = portugal_municipalities.plot()\n",
    "    municipality_gdf.plot(ax=pt_map, color='yellow')\n",
    "    point_df.plot(ax=pt_map, color='red', markersize=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "municipality_data = get_municipality(point=user_point)\n",
    "plot_point_in_map(user_point, municipality_data[\"municipality_gdf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As mentioned before, we visualized data correlation in several ways. The script below saved several figures that would later be used in our report to illustrate the prevalence of each class of occurrence in each district."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:06.195494Z",
     "end_time": "2023-04-23T10:28:10.340694Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#Parses the data and makes list of lists based on unique values for the Districts columns\n",
    "def occurrence_frequency(df: pd.DataFrame, categorylevel: str, geoscope: str, datasetisclean = False):\n",
    "    \n",
    "    if not datasetisclean:\n",
    "        df = clean_data(df)\n",
    "    \n",
    "    geoscopef = geoscope\n",
    "    geo_list = []\n",
    "    abreviation_dict = {'Assistência e Prevenção a actividades humanas' : 1, 'Comprometimento total ou parcial de segurança, serviços ou estruturas': 2,\n",
    "                        'Incêndios Urbanos ou em Área Urbanizável':3, 'Incêndios em Equipamento e Produtos':4}\n",
    "\n",
    "    for row1 in df[geoscopef].unique():\n",
    "        cat_list = []\n",
    "        i = 0\n",
    "        #Parses the Districts columns to count occurrences\n",
    "        for row2 in df[geoscopef]:\n",
    "            if row1 == row2:\n",
    "                category = df[categorylevel].iloc[i]\n",
    "                #Abbreviates some categories for better visualization graphs\n",
    "                if category in abreviation_dict:\n",
    "                    value = abreviation_dict[category]\n",
    "                    if value == 1:\n",
    "                        category = \"Assist e Prev a actv humanas\"\n",
    "                    elif value == 2:\n",
    "                        category = \"Comp total ou parcial de seg\"\n",
    "                    elif value == 3:\n",
    "                        category = \"Incêndios urbanos\"\n",
    "                    elif value == 4:\n",
    "                        category = \"Incêndio em equipm\"\n",
    "                #Verifies if category is in list and adds to occurrence count if it does, adds category to list if it doesn't\n",
    "                if category in cat_list:\n",
    "                    cat_list.index(category)\n",
    "                    cat_list[cat_list.index(category)+1] = cat_list[cat_list.index(category)+1] + 1\n",
    "                else:\n",
    "                    cat_list.append(category)\n",
    "                    cat_list.append(1)\n",
    "            i = i + 1\n",
    "        geo_list.append(cat_list)\n",
    "        \n",
    "\n",
    "    # Transform the elements in each sublist into tuples of two\n",
    "    list_of_tuples = [[(lst[2*i], lst[2*i+1]) for i in range(round(len(lst)/2-1))] for lst in geo_list]\n",
    "\n",
    "    #Define a counter and makes relation between occurrences and Districts for visualization of data\n",
    "    j = 0\n",
    "    for row1 in df[geoscopef].unique():\n",
    "        list_of_tuples[j] = sorted(list_of_tuples[j], key = lambda x: (x[0]))\n",
    "        # Separate the x and y values into two lists\n",
    "        x_values = [x[0] for x in list_of_tuples[j]]\n",
    "        y_values = [x[1] for x in list_of_tuples[j]]\n",
    "        # Plot the data\n",
    "        plt.bar(x_values, y_values)\n",
    "        plt.title(\"Breakdown of occurrences for\" + \" \" + str(row1))\n",
    "        plt.xlabel(\"occurrence type\")\n",
    "        plt.xticks(fontsize=8,rotation=45,ha='right')\n",
    "        plt.ylabel(\"Number of occurrences\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('.','images',f'{row1}_occurrences_{geoscope}_{categorylevel}.png'))\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        j = j+1\n",
    "        \n",
    "    return list_of_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "As well as visualizing the most prevalent issue in each administrative subdivision on a map."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:06.198788Z",
     "end_time": "2023-04-23T10:28:10.365273Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import colormaps\n",
    "import matplotlib.patches as mpatches\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "# Normalize name\n",
    "def normalize_str(text: str) -> str:\n",
    "    return unidecode(text).upper()\n",
    "\n",
    "\n",
    "# Show the most prevalent issue category in each municipality\n",
    "def plot_most_prevalent_issue_by_municipality(\n",
    "        clean_df: pd.DataFrame,\n",
    "        category_level: int = 2\n",
    "):\n",
    "    issue_by_municipality = {}\n",
    "    for municipality in clean_df[\"Concelho\"].unique():\n",
    "        clean_df_by_municipality = clean_df[\n",
    "            clean_df[\"Concelho\"] == municipality\n",
    "        ]\n",
    "        most_prevalent_issue = clean_df_by_municipality.mode()[\n",
    "            f\"category{category_level}\"\n",
    "        ][0]\n",
    "        issue_by_municipality[municipality] = most_prevalent_issue\n",
    "\n",
    "    # Declare the color map that will be used\n",
    "    color_map_names = [\"viridis\", \"plasma\", \"cividis\"]\n",
    "    base_color_map = colormaps[color_map_names[category_level - 1]].resampled(\n",
    "        len(set(issue_by_municipality.values()))\n",
    "    )\n",
    "    color_by_issue = {\n",
    "        issue_name: base_color_map.colors[idx]\n",
    "        for idx, issue_name in enumerate(set(issue_by_municipality.values()))\n",
    "    }\n",
    "\n",
    "    # Get shape of municipalities and parishes\n",
    "    portugal_municipalities = gpd.read_file(\n",
    "        \"../georeferencing/data_concelhos/concelhos.shp\"\n",
    "    )\n",
    "    # Remove islands\n",
    "    portugal_municipalities = portugal_municipalities[\n",
    "        portugal_municipalities.NAME_1 != \"Azores\"\n",
    "    ]\n",
    "    portugal_municipalities = portugal_municipalities[\n",
    "        portugal_municipalities.NAME_1 != \"Madeira\"\n",
    "    ]\n",
    "    # Normalize municipality name\n",
    "    portugal_municipalities[\"NAME_2\"] = (\n",
    "        portugal_municipalities[\"NAME_2\"].astype(str).apply(normalize_str)\n",
    "    )\n",
    "\n",
    "    pt_map = portugal_municipalities.plot(color=\"gray\")\n",
    "\n",
    "    for municipality_name, issue_name in issue_by_municipality.items():\n",
    "        matching_municipalities = portugal_municipalities.query(\n",
    "            f\"NAME_2=='{normalize_str(municipality_name)}'\"\n",
    "        )\n",
    "        for _, row in matching_municipalities.iterrows():\n",
    "            municipality_gdf = gpd.GeoDataFrame([row], crs=portugal_municipalities.crs)\n",
    "            municipality_gdf.plot(ax=pt_map, color=color_by_issue[issue_name])\n",
    "\n",
    "    pt_map.legend(\n",
    "        handles=[\n",
    "            # mpatches.Patch(color=\"gray\", label=\"Sem dados\"),  # We have all of it!\n",
    "            *[\n",
    "                mpatches.Patch(color=base_color_map.colors[idx], label=issue_name)\n",
    "                for idx, issue_name in enumerate(set(issue_by_municipality.values()))\n",
    "            ],\n",
    "        ],\n",
    "        loc=\"lower left\",\n",
    "        bbox_to_anchor=(0.1, 1),\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we created a random forest classifier to evaluate the probability of each occurrence on a specific date in each municipality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:06.201815Z",
     "end_time": "2023-04-23T10:28:10.714955Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Data/anpc-2016.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T10:28:06.201898Z",
     "end_time": "2023-04-23T10:28:14.040469Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "occurrences = parse_categories(clean_data(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T09:16:35.078941Z",
     "end_time": "2023-04-23T09:16:36.759272Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Get relevant and error-free data\n",
    "occurrences = parse_categories(clean_data(occurrences))\n",
    "\n",
    "occurrences[\"day_idx\"] = occurrences[\"DataOcorrencia\"] - min(\n",
    "    occurrences[\"DataOcorrencia\"]\n",
    ")\n",
    "occurrences[\"day_idx\"] = occurrences[\"day_idx\"].apply(\n",
    "    lambda occ: occ.total_seconds() / 3600\n",
    ")\n",
    "\n",
    "# encode the categorical variables\n",
    "le_state = LabelEncoder()\n",
    "region = \"Distrito\"\n",
    "occurrences[region] = le_state.fit_transform(occurrences[region])\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "encoded_data = enc.fit_transform(occurrences[[region]])\n",
    "\n",
    "# combine the encoded variables with the numerical variables\n",
    "X = pd.concat(\n",
    "    [pd.DataFrame(encoded_data).reset_index(inplace=True), occurrences[\"day_idx\"]],\n",
    "    axis=1,\n",
    ")\n",
    "print(X)\n",
    "y = occurrences[\"category2\"]\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=12\n",
    ")\n",
    "\n",
    "# train a random forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=1000, random_state=12)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy score: {accuracy:.2f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
