{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Import necessary libraries and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project.preprocess_data import clean_data, parse_categories\n",
    "from project.linear_regression import hashed_reg\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def datetime_to_hours_int(x):\n",
    "    return [i.total_seconds()/3600 for i in x]\n",
    "\n",
    "def clean_data(df, remove_false_alarms=True) -> pd.DataFrame:\n",
    "    print('df initial length: ', len(df))\n",
    "\n",
    "    data_clean = df[df['Natureza'] != '1']\n",
    "    data_clean = data_clean[data_clean['EstadoOcorrencia'] != '2']\n",
    "    data_clean = data_clean[data_clean['Distrito'] != '0']\n",
    "    data_clean = data_clean[data_clean['Concelho'] != '0']\n",
    "    data_clean = data_clean[data_clean['Numero'] != \"\"\"\\t\\t\\t\\t\\t\\t\\t\\t\"\"\"]\n",
    "    # data_clean = data_clean[data_clean['EstadoOcorrencia'] != 'Falso Alerta']\n",
    "    # data_clean = data_clean[data_clean['EstadoOcorrencia'] != 'Falso Alarme']\n",
    "    data_clean = pd.concat([data_clean[data_clean['EstadoOcorrencia'] == 'Encerrada'], data_clean[data_clean['EstadoOcorrencia'] == 'Falso Alarme']])\n",
    "\n",
    "    # We replace the \",\" with \".\" to facilitate processing\n",
    "    data_clean['Latitude'] = pd.to_numeric(\n",
    "        data_clean['Latitude'].str.replace(',', '.')\n",
    "    )\n",
    "    data_clean['Longitude'] = pd.to_numeric(\n",
    "        data_clean['Longitude'].str.replace(',', '.')\n",
    "    )    \n",
    "\n",
    "    print(data_clean['Longitude'][0], ' :', type(data_clean['Longitude'][0]))\n",
    "    print(data_clean['Latitude'][0], ' :', type(data_clean['Latitude'][0]))\n",
    "\n",
    "    data_clean['DataOcorrencia'] = pd.to_datetime(data_clean['DataOcorrencia'], format='%d/%m/%Y %H:%M:%S')\n",
    "    data_clean['DataFechoOperacional'] = pd.to_datetime(data_clean['DataFechoOperacional'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "    print(data_clean['DataOcorrencia'][0], ' :', type(data_clean['DataOcorrencia'][0]))\n",
    "    print(data_clean['DataFechoOperacional'][0], ' :', type(data_clean['DataFechoOperacional'][0]))\n",
    "\n",
    "    if remove_false_alarms == True:\n",
    "        data_clean = data_clean[data_clean['EstadoOcorrencia'] != 'Falso Alarme']\n",
    "\n",
    "    data_clean.dropna(inplace=True)\n",
    "    print('df length: ', len(data_clean))\n",
    "\n",
    "    data_clean['hh'] = np.multiply(\n",
    "        (data_clean['NumeroOperacionaisAereosEnvolvidos'] + data_clean['NumeroOperacionaisTerrestresEnvolvidos']),\n",
    "        datetime_to_hours_int(data_clean['DataFechoOperacional'] - data_clean['DataOcorrencia'])\n",
    "    )\n",
    "\n",
    "    return data_clean\n",
    "\n",
    "    # Define a function to extract category from a string\n",
    "def _extract_category(string, cat_index):\n",
    "    if string == \"nan\" or any(str.isdigit(c) for c in string):\n",
    "        return\n",
    "    else:\n",
    "        parts = string.split(\"/\", 4)\n",
    "        return parts[cat_index-1].strip()\n",
    "\n",
    "\n",
    "#Reformulate categories based on functions defined\n",
    "def parse_categories(data_clean:pd.DataFrame):\n",
    "    data_clean['category1'] = data_clean['Natureza'].astype(str).apply(lambda x: _extract_category(x,cat_index=1))\n",
    "    data_clean['category2'] = data_clean['Natureza'].astype(str).apply(lambda x: _extract_category(x,cat_index=2))\n",
    "    data_clean['category3'] = data_clean['Natureza'].astype(str).apply(lambda x: _extract_category(x,cat_index=3))\n",
    "    \n",
    "    #Consolidate categories with little statistical relevance\n",
    "    relevance_threshold = 100\n",
    "    category_counts = data_clean['category3'].value_counts()\n",
    "    filtered_categories = category_counts[category_counts <= relevance_threshold].index.tolist()\n",
    "    # Replace the filtered categories with a new \"Other\" category\n",
    "    data_clean['category3'] = data_clean['category3'].replace(\n",
    "    to_replace=filtered_categories,\n",
    "    value='Outras ocorrências'\n",
    "    )\n",
    "    \n",
    "    return data_clean\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Read the database .CSV\n",
    "occurrences = pd.read_csv(\n",
    "    \"Data/anpc-2016.csv\", sep=\",\", on_bad_lines=\"skip\", low_memory=False\n",
    ")\n",
    "\n",
    "# Get relevant and error-free data\n",
    "occurrences = parse_categories(clean_data(occurrences))\n",
    "\n",
    "def hashed_reg(occurrences):\n",
    "    # Encode the categorical variables using feature hashing with HashingVectorizer\n",
    "    num_districts = occurrences.nunique()[\"Distrito\"]\n",
    "    hash_size_increase = 2.5  # Increase number of features to avoid hash collision\n",
    "    district_vectorizer = HashingVectorizer(n_features=num_districts, norm=None, alternate_sign=False)\n",
    "    X_cat = district_vectorizer.fit_transform(occurrences[\"Distrito\"])\n",
    "\n",
    "    num_categories = occurrences.nunique()[\"category2\"]\n",
    "    hash_size_increase = 2.5  # Increase number of features to avoid hash collision\n",
    "    category_vectorizer = HashingVectorizer(n_features=num_categories, norm=None, alternate_sign=False)\n",
    "    X_cat2 = category_vectorizer.fit_transform(occurrences[\"category2\"])\n",
    "\n",
    "    # Combine the encoded categorical variables and the numerical variable\n",
    "    X = pd.concat(\n",
    "        [\n",
    "        pd.DataFrame(X_cat.toarray()), \n",
    "        pd.DataFrame(X_cat2.toarray()),\n",
    "        pd.DataFrame(datetime_to_hours_int(occurrences['DataOcorrencia'] - min(occurrences['DataOcorrencia'])))\n",
    "        ], axis=1\n",
    "    )\n",
    "\n",
    "    # Fit a linear regression model to the data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, occurrences['hh'])\n",
    "\n",
    "    return model.predict(X)\n",
    "\n",
    "def hashed_reg_2(occurrences):\n",
    "    num_districts = occurrences.nunique()[\"Distrito\"]\n",
    "    hash_size_increase = 2.5  # Increase number of features to avoid hash collision\n",
    "    h = FeatureHasher(n_features=int(hash_size_increase * num_districts), input_type=\"string\")\n",
    "    return h.transform(list(occurrences[\"Distrito\"]))\n",
    "\n",
    "hashed_reg(occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# shp -> shapefile (geometry)\n",
    "# dbf -> database (extra info)\n",
    "# prj - projection\n",
    "# shx - index\n",
    "\n",
    "# Get shape of municipalities and parishes\n",
    "portugal_municipalities = gpd.read_file(\"data_concelhos/concelhos.shp\")\n",
    "\n",
    "# Remove islands\n",
    "portugal_municipalities = portugal_municipalities[portugal_municipalities.NAME_1 != \"Azores\"]\n",
    "portugal_municipalities = portugal_municipalities[portugal_municipalities.NAME_1 != \"Madeira\"]\n",
    "\n",
    "# Get point defined by user\n",
    "lat, lon = 40.9800516, -8.202641\n",
    "user_point = Point(lon, lat)\n",
    "\n",
    "\n",
    "def get_municipality(point: Point):\n",
    "    for index, row in portugal_municipalities.iterrows():\n",
    "        polygon = row['geometry']\n",
    "        # Check if the point is inside the polygon\n",
    "        if polygon.contains(point):\n",
    "            # Return the index or other properties of the polygon\n",
    "            return {\n",
    "                \"district\": row.NAME_1,\n",
    "                \"municipality\": row.NAME_2,\n",
    "                \"municipality_gdf\": gpd.GeoDataFrame([row], crs=portugal_municipalities.crs)\n",
    "            }\n",
    "\n",
    "\n",
    "def plot_point_in_map(point, municipality_gdf):\n",
    "    point_df = gpd.GeoDataFrame(geometry=[point])\n",
    "    pt_map = portugal_municipalities.plot()\n",
    "    municipality_gdf.plot(ax=pt_map, color='yellow')\n",
    "    point_df.plot(ax=pt_map, color='red', markersize=10)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "municipality_data = get_municipality(point=user_point)\n",
    "plot_point_in_map(user_point, municipality_data[\"municipality_gdf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocess_data import clean_data\n",
    "import os\n",
    "\n",
    "#Parses the data and makes list of lists based on unique values for the Districts columns\n",
    "def ocurrence_frequency(df:pd.DataFrame,categorylevel:str,geoscope:str,datasetisclean = False):\n",
    "    \n",
    "    if datasetisclean == False:\n",
    "        ocorrencias = clean_data(ocorrencias)\n",
    "    \n",
    "    geoscopef = geoscope\n",
    "    geo_list = []\n",
    "    abreviation_dict = {'Assistência e Prevenção a actividades humanas' : 1, 'Comprometimento total ou parcial de segurança, serviços ou estruturas': 2,\n",
    "                        'Incêndios Urbanos ou em Área Urbanizável':3, 'Incêndios em Equipamento e Produtos':4}\n",
    "\n",
    "    for row1 in df[geoscopef].unique():\n",
    "        cat_list = []\n",
    "        i = 0\n",
    "        #Parses the Districts columns to count ocurrences\n",
    "        for row2 in df[geoscopef]:\n",
    "            if row1 == row2:\n",
    "                category = df[categorylevel].iloc[i]\n",
    "                #Abbreviates some categories for better visualization graphs\n",
    "                if category in abreviation_dict:\n",
    "                    value = abreviation_dict[category]\n",
    "                    if value == 1:\n",
    "                        category = \"Assist e Prev a actv humanas\"\n",
    "                    elif value == 2:\n",
    "                        category = \"Comp total ou parcial de seg\"\n",
    "                    elif value == 3:\n",
    "                        category = \"Incêndios urbanos\"\n",
    "                    elif value == 4:\n",
    "                        category = \"Incêndio em equipm\"\n",
    "                #Verifies if category is in list and adds to occurrence count if it does, adds category to list if it doesn't\n",
    "                if category in cat_list:\n",
    "                    cat_list.index(category)\n",
    "                    cat_list[cat_list.index(category)+1] = cat_list[cat_list.index(category)+1] + 1\n",
    "                else:\n",
    "                    cat_list.append(category)\n",
    "                    cat_list.append(1)\n",
    "            i = i + 1\n",
    "        geo_list.append(cat_list)\n",
    "        \n",
    "\n",
    "    # Transform the elements in each sublist into tuples of two\n",
    "    list_of_tuples = [[(lst[2*i], lst[2*i+1]) for i in range(round(len(lst)/2-1))] for lst in geo_list]\n",
    "\n",
    "    #Define a counter and makes relation between Ocurrences and Districts for visualization of data\n",
    "    j = 0\n",
    "    for row1 in df[geoscopef].unique():\n",
    "        list_of_tuples[j] = sorted(list_of_tuples[j], key = lambda x: (x[0]))\n",
    "    # Separate the x and y values into two lists\n",
    "        x_values = [x[0] for x in list_of_tuples[j]]\n",
    "        y_values = [x[1] for x in list_of_tuples[j]]\n",
    "    # Plot the data\n",
    "        plt.bar(x_values, y_values)\n",
    "        plt.title(\"Breakdown of ocurrences for\" + \" \" + str(row1))\n",
    "        plt.xlabel(\"Ocurrence type\")\n",
    "        plt.xticks(fontsize=8,rotation=45,ha='right')\n",
    "        plt.ylabel(\"Number of ocurrences\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join('.','images',f'{row1}_ocurrences_{geoscope}_{categorylevel}.png'))\n",
    "        plt.clf()\n",
    "        plt.cla()\n",
    "        j = j+1\n",
    "        \n",
    "    return list_of_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colormaps\n",
    "import matplotlib.patches as mpatches\n",
    "from utils import normalize_str\n",
    "\n",
    "\n",
    "# Show the most prevalent issue category in each municipality\n",
    "def plot_most_prevalent_issue_by_municipality(\n",
    "        clean_df: pd.DataFrame,\n",
    "        category_level: int = 2\n",
    "):\n",
    "    issue_by_municipality = {}\n",
    "    for municipality in clean_df[\"Concelho\"].unique():\n",
    "        clean_df_by_municipality = clean_df[\n",
    "            clean_df[\"Concelho\"] == municipality\n",
    "        ]\n",
    "        most_prevalent_issue = clean_df_by_municipality.mode()[\n",
    "            f\"category{category_level}\"\n",
    "        ][0]\n",
    "        issue_by_municipality[municipality] = most_prevalent_issue\n",
    "\n",
    "    # Declare the color map that will be used\n",
    "    color_map_names = [\"viridis\", \"plasma\", \"cividis\"]\n",
    "    base_color_map = colormaps[color_map_names[category_level - 1]].resampled(\n",
    "        len(set(issue_by_municipality.values()))\n",
    "    )\n",
    "    color_by_issue = {\n",
    "        issue_name: base_color_map.colors[idx]\n",
    "        for idx, issue_name in enumerate(set(issue_by_municipality.values()))\n",
    "    }\n",
    "\n",
    "    # Get shape of municipalities and parishes\n",
    "    portugal_municipalities = gpd.read_file(\n",
    "        \"../georeferencing/data_concelhos/concelhos.shp\"\n",
    "    )\n",
    "    # Remove islands\n",
    "    portugal_municipalities = portugal_municipalities[\n",
    "        portugal_municipalities.NAME_1 != \"Azores\"\n",
    "    ]\n",
    "    portugal_municipalities = portugal_municipalities[\n",
    "        portugal_municipalities.NAME_1 != \"Madeira\"\n",
    "    ]\n",
    "    # Normalize municipality name\n",
    "    portugal_municipalities[\"NAME_2\"] = (\n",
    "        portugal_municipalities[\"NAME_2\"].astype(str).apply(normalize_str)\n",
    "    )\n",
    "\n",
    "    pt_map = portugal_municipalities.plot(color=\"gray\")\n",
    "\n",
    "    for municipality_name, issue_name in issue_by_municipality.items():\n",
    "        matching_municipalities = portugal_municipalities.query(\n",
    "            f\"NAME_2=='{normalize_str(municipality_name)}'\"\n",
    "        )\n",
    "        for _, row in matching_municipalities.iterrows():\n",
    "            municipality_gdf = gpd.GeoDataFrame([row], crs=portugal_municipalities.crs)\n",
    "            municipality_gdf.plot(ax=pt_map, color=color_by_issue[issue_name])\n",
    "\n",
    "    pt_map.legend(\n",
    "        handles=[\n",
    "            # mpatches.Patch(color=\"gray\", label=\"Sem dados\"),  # We have all of it!\n",
    "            *[\n",
    "                mpatches.Patch(color=base_color_map.colors[idx], label=issue_name)\n",
    "                for idx, issue_name in enumerate(set(issue_by_municipality.values()))\n",
    "            ],\n",
    "        ],\n",
    "        loc=\"lower left\",\n",
    "        bbox_to_anchor=(0.1, 1),\n",
    "    )\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_88010/1922025828.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(\"Data/anpc-2016.csv\")\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"Data/anpc-2016.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df initial length:  121187\n",
      "-9.002235449  : <class 'numpy.float64'>\n",
      "38.68091202  : <class 'numpy.float64'>\n",
      "2016-01-09 14:02:00  : <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "2016-01-09 17:30:00  : <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "df length:  118168\n"
     ]
    }
   ],
   "source": [
    "clean_dataset = parse_categories(clean_data(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = hashed_reg(occurrences=clean_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002740760194571701"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(clean_dataset['hh'], y_predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
